=============================================类型：Segmentation fault,无error结尾==================================================================
(WorkerDict pid=7435, ip=172.16.22.183) [2025-11-24 22:27:20,620 E 7435 7435] logging.cc:361:     @ ... and at least 178 more frames
(WorkerDict pid=7435, ip=172.16.22.183) Fatal Python error: Segmentation fault
(WorkerDict pid=7435, ip=172.16.22.183)
(WorkerDict pid=7435, ip=172.16.22.183) Stack (most recent call first):
(WorkerDict pid=7435, ip=172.16.22.183)   File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527 in _call_impl
(WorkerDict pid=7435, ip=172.16.22.183)   File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518 in _wrapped_call_impl
(WorkerDict pid=7435, ip=172.16.22.183)   File "/home/ma-user/modelarts/user-job-dir/pangu_rft_data_sampler/resources/tokenizer/pangu_vl/pangu_pi_model.py", line 327 in forward
(WorkerDict pid=7428, ip=172.16.22.183) INFO:vllm_adapter.vllm_v_0_6_3.spmd_gpu_executor:# GPU blocks: 9963, # CPU blocks: 4818
(WorkerDict pid=7425, ip=172.16.22.183) INFO:vllm_adapter.vllm_v_0_6_3.spmd_gpu_executor:# GPU blocks: 9963, # CPU blocks: 4818

=====================================================类型：Traceback前有相同前缀，有结尾error==========================================================
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]: Traceback (most recent call last):
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:   File "/cache/algorithm/mindspeed_llm/training/training.py", line 558, in pretrain
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:     iteration, num_floating_point_operations_so_far = train(*train_args)
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:   File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/mindspore/common/tensor.py", line 888, in assign_value
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:     self.assign_value_cpp(value)
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]: RuntimeError: MemcpyAsync failed!
/opt/huawei/schedule-train/log/plog/worker_1953.log-11474--------------------------------------------------
=====================================================类型：Traceback前有相同前缀，无结尾error==========================================================
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]: Traceback (most recent call last):
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:   File "/cache/algorithm/mindspeed_llm/training/training.py", line 558, in pretrain
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:     iteration, num_floating_point_operations_so_far = train(*train_args)
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:   File "/cache/algorithm/mindspeed_llm/training/training.py", line 757, in train
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:     self.assign_value(new_value)
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:   File "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/mindspore/common/tensor.py", line 888, in assign_value
/opt/huawei/schedule-train/log/plog/worker_1953.log-11454-[rank1953]:     self.assign_value_cpp(value)
/opt/huawei/schedule-train/log/plog/worker_1953.log-11474-------------------------------------------------
11111111111111111111111111111111
2222222222222